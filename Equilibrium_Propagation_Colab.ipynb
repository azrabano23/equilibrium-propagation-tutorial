{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Equilibrium Propagation Tutorial\n",
        "\n",
        "This notebook implements **Equilibrium Propagation**, a learning algorithm for energy-based models proposed by Scellier & Bengio (2017).\n",
        "\n",
        "## Key Concepts:\n",
        "- **Energy-based Models**: Networks that define an energy function over states\n",
        "- **Free Phase**: Network relaxes to equilibrium without target clamping\n",
        "- **Weakly Clamped Phase**: Network relaxes with weak target signal\n",
        "- **Learning Rule**: Parameter updates based on energy differences between phases\n",
        "\n",
        "**Paper**: [Equilibrium Propagation: Bridging the Gap between Energy-Based Models and Backpropagation](https://arxiv.org/abs/1602.05179)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    device = 'cuda'\nelse:\n",
        "    device = 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dependencies"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "network_class"
      },
      "outputs": [],
      "source": [
        "class EquilibriumPropagationNetwork:\n",
        "    \"\"\"Modern PyTorch implementation of Equilibrium Propagation\n",
        "    \n",
        "    Based on the paper \"Equilibrium Propagation: Bridging the Gap between \n",
        "    Energy-Based Models and Backpropagation\" by Scellier & Bengio (2017)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_sizes=[500], device='cpu'):\n",
        "        self.device = device\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        \n",
        "        # Network architecture: 784 (MNIST) -> hidden_sizes -> 10\n",
        "        layer_sizes = [784] + hidden_sizes + [10]\n",
        "        \n",
        "        # Initialize weights and biases\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            # Glorot/Xavier initialization\n",
        "            fan_in, fan_out = layer_sizes[i], layer_sizes[i+1]\n",
        "            bound = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "            W = torch.empty(fan_in, fan_out).uniform_(-bound, bound).to(device)\n",
        "            W.requires_grad_(True)\n",
        "            self.weights.append(W)\n",
        "            \n",
        "            # Initialize biases to zero\n",
        "            b = torch.zeros(layer_sizes[i+1]).to(device)\n",
        "            b.requires_grad_(True)\n",
        "            self.biases.append(b)\n",
        "    \n",
        "    def rho(self, s):\n",
        "        \"\"\"Hard sigmoid activation function: rho(s) = max(0, min(1, s))\"\"\"\n",
        "        return torch.clamp(s, 0.0, 1.0)\n",
        "    \n",
        "    def energy(self, layers):\n",
        "        \"\"\"Compute energy function E for the current state\"\"\"\n",
        "        # Squared norm term: sum(rho(layer)^2) / 2\n",
        "        squared_norm = sum([(self.rho(layer) ** 2).sum(dim=1) for layer in layers]) / 2.0\n",
        "        \n",
        "        # Linear terms: -sum(rho(layer) * bias) - note: skip input layer for biases\n",
        "        linear_terms = -sum([torch.sum(self.rho(layer) * bias, dim=1) \n",
        "                           for layer, bias in zip(layers[1:], self.biases)])\n",
        "        \n",
        "        # Quadratic terms: -sum(rho(pre) @ W @ rho(post))\n",
        "        quadratic_terms = -sum([torch.sum(self.rho(pre) @ W * self.rho(post), dim=1)\n",
        "                               for pre, W, post in zip(layers[:-1], self.weights, layers[1:])])\n",
        "        \n",
        "        return squared_norm + linear_terms + quadratic_terms\n",
        "    \n",
        "    def cost(self, output_layer, target):\n",
        "        \"\"\"Compute cost function C (mean squared error)\"\"\"\n",
        "        target_one_hot = F.one_hot(target, num_classes=10).float()\n",
        "        return ((output_layer - target_one_hot) ** 2).sum(dim=1)\n",
        "    \n",
        "    def total_energy(self, layers, target, beta):\n",
        "        \"\"\"Compute total energy F = E + beta * C\"\"\"\n",
        "        return self.energy(layers) + beta * self.cost(layers[-1], target)\n",
        "    \n",
        "    def free_phase(self, x, n_iterations=20, epsilon=0.5):\n",
        "        \"\"\"Run free phase dynamics to find equilibrium\"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # Initialize layers\n",
        "        layers = [x]  # Input layer (clamped)\n",
        "        for size in self.hidden_sizes + [10]:\n",
        "            layer = torch.zeros(batch_size, size, requires_grad=True, device=self.device)\n",
        "            layers.append(layer)\n",
        "        \n",
        "        # Run dynamics\n",
        "        for _ in range(n_iterations):\n",
        "            # Compute energy gradient\n",
        "            energy_val = self.energy(layers).sum()\n",
        "            grads = torch.autograd.grad(energy_val, layers[1:], create_graph=True)\n",
        "            \n",
        "            # Update layers (except input which is clamped)\n",
        "            new_layers = [layers[0]]  # Keep input layer\n",
        "            for i, grad in enumerate(grads):\n",
        "                new_layer = self.rho(layers[i+1] - epsilon * grad)\n",
        "                new_layer.requires_grad_(True)\n",
        "                new_layers.append(new_layer)\n",
        "            layers = new_layers\n",
        "        \n",
        "        return layers\n",
        "    \n",
        "    def weakly_clamped_phase(self, x, target, n_iterations=4, epsilon=0.5, beta=0.5):\n",
        "        \"\"\"Run weakly clamped phase dynamics\"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # Initialize layers\n",
        "        layers = [x]  # Input layer (clamped)\n",
        "        for size in self.hidden_sizes + [10]:\n",
        "            layer = torch.zeros(batch_size, size, requires_grad=True, device=self.device)\n",
        "            layers.append(layer)\n",
        "        \n",
        "        # Run dynamics\n",
        "        for _ in range(n_iterations):\n",
        "            # Compute total energy gradient\n",
        "            total_energy_val = self.total_energy(layers, target, beta).sum()\n",
        "            grads = torch.autograd.grad(total_energy_val, layers[1:], create_graph=True)\n",
        "            \n",
        "            # Update layers (except input which is clamped)\n",
        "            new_layers = [layers[0]]  # Keep input layer\n",
        "            for i, grad in enumerate(grads):\n",
        "                new_layer = self.rho(layers[i+1] - epsilon * grad)\n",
        "                new_layer.requires_grad_(True)\n",
        "                new_layers.append(new_layer)\n",
        "            layers = new_layers\n",
        "        \n",
        "        return layers\n",
        "    \n",
        "    def compute_gradients(self, layers_free, layers_clamped, beta):\n",
        "        \"\"\"Compute parameter gradients using equilibrium propagation\"\"\"\n",
        "        # Energy at free equilibrium\n",
        "        energy_free = self.energy(layers_free).mean()\n",
        "        \n",
        "        # Energy at weakly clamped equilibrium  \n",
        "        energy_clamped = self.energy(layers_clamped).mean()\n",
        "        \n",
        "        # Gradient of energy difference w.r.t. parameters\n",
        "        energy_diff = (energy_clamped - energy_free) / beta\n",
        "        \n",
        "        weight_grads = torch.autograd.grad(energy_diff, self.weights, retain_graph=True)\n",
        "        bias_grads = torch.autograd.grad(energy_diff, self.biases)\n",
        "        \n",
        "        return weight_grads, bias_grads\n",
        "    \n",
        "    def update_parameters(self, weight_grads, bias_grads, alphas):\n",
        "        \"\"\"Update network parameters\"\"\"\n",
        "        with torch.no_grad():\n",
        "            for i, (W, grad) in enumerate(zip(self.weights, weight_grads)):\n",
        "                W -= alphas[i] * grad\n",
        "            \n",
        "            for i, (b, grad) in enumerate(zip(self.biases, bias_grads)):\n",
        "                b -= alphas[i] * grad\n",
        "    \n",
        "    def predict(self, x):\n",
        "        \"\"\"Make predictions using free phase\"\"\"\n",
        "        layers = self.free_phase(x)\n",
        "        return torch.argmax(layers[-1], dim=1)\n",
        "    \n",
        "    def measure(self, x, target):\n",
        "        \"\"\"Measure energy, cost, and error rate\"\"\"\n",
        "        layers = self.free_phase(x)\n",
        "        \n",
        "        energy_val = self.energy(layers).mean()\n",
        "        cost_val = self.cost(layers[-1], target).mean()\n",
        "        \n",
        "        predictions = torch.argmax(layers[-1], dim=1)\n",
        "        error_rate = (predictions != target).float().mean()\n",
        "        \n",
        "        return energy_val.item(), cost_val.item(), error_rate.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loading"
      },
      "outputs": [],
      "source": [
        "def load_mnist_data(batch_size=20):\n",
        "    \"\"\"Load MNIST dataset\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.view(-1))  # Flatten 28x28 to 784\n",
        "    ])\n",
        "    \n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Load data\n",
        "print(\"Loading MNIST dataset...\")\n",
        "train_loader, test_loader = load_mnist_data(batch_size=20)\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo_run"
      },
      "outputs": [],
      "source": [
        "# Quick demo with a single batch\n",
        "print(\"Creating network...\")\n",
        "net = EquilibriumPropagationNetwork(hidden_sizes=[100], device=device)  # Smaller for demo\n",
        "\n",
        "# Get a single batch\n",
        "data_iter = iter(train_loader)\n",
        "data, target = next(data_iter)\n",
        "data, target = data.to(device), target.to(device)\n",
        "\n",
        "print(f\"Batch shape: {data.shape}\")\n",
        "print(f\"Target shape: {target.shape}\")\n",
        "\n",
        "# Test free phase\n",
        "print(\"\\nRunning free phase...\")\n",
        "start_time = time.time()\n",
        "layers_free = net.free_phase(data, n_iterations=10, epsilon=0.5)\n",
        "free_time = time.time() - start_time\n",
        "print(f\"Free phase took {free_time:.2f} seconds\")\n",
        "\n",
        "# Test weakly clamped phase\n",
        "print(\"\\nRunning weakly clamped phase...\")\n",
        "start_time = time.time()\n",
        "layers_clamped = net.weakly_clamped_phase(data, target, n_iterations=4, epsilon=0.5, beta=0.5)\n",
        "clamped_time = time.time() - start_time\n",
        "print(f\"Weakly clamped phase took {clamped_time:.2f} seconds\")\n",
        "\n",
        "# Measure performance\n",
        "energy, cost, error = net.measure(data, target)\n",
        "print(f\"\\nInitial performance:\")\n",
        "print(f\"Energy: {energy:.3f}\")\n",
        "print(f\"Cost: {cost:.3f}\")\n",
        "print(f\"Error rate: {error*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_function"
      },
      "outputs": [],
      "source": [
        "def train_network(hidden_sizes=[100], n_epochs=3, batch_size=20, \n",
        "                 n_it_neg=10, n_it_pos=4, epsilon=0.5, beta=0.5, \n",
        "                 alphas=[0.1, 0.05], device='cpu', max_batches=50):\n",
        "    \"\"\"Train equilibrium propagation network (simplified for demo)\"\"\"\n",
        "    \n",
        "    print(f\"Architecture: 784-{'-'.join(map(str, hidden_sizes))}-10\")\n",
        "    print(f\"Epochs: {n_epochs}, Batch size: {batch_size}\")\n",
        "    print(f\"Free phase iterations: {n_it_neg}, Clamped phase iterations: {n_it_pos}\")\n",
        "    print(f\"Learning rate: {epsilon}, Beta: {beta}\")\n",
        "    print(f\"Alphas: {alphas}\")\n",
        "    print(f\"Max batches per epoch: {max_batches}\\n\")\n",
        "    \n",
        "    # Initialize network\n",
        "    net = EquilibriumPropagationNetwork(hidden_sizes, device)\n",
        "    \n",
        "    # Load data\n",
        "    train_loader, test_loader = load_mnist_data(batch_size)\n",
        "    \n",
        "    # Training curves\n",
        "    training_errors = []\n",
        "    validation_errors = []\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        \n",
        "        # Training phase\n",
        "        train_error_sum = 0.0\n",
        "        num_train_batches = 0\n",
        "        \n",
        "        pbar = tqdm(enumerate(train_loader), total=min(max_batches, len(train_loader)))\n",
        "        for batch_idx, (data, target) in pbar:\n",
        "            if batch_idx >= max_batches:\n",
        "                break\n",
        "                \n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Free phase\n",
        "            layers_free = net.free_phase(data, n_it_neg, epsilon)\n",
        "            \n",
        "            # Measure at free equilibrium\n",
        "            _, _, error = net.measure(data, target)\n",
        "            train_error_sum += error\n",
        "            num_train_batches += 1\n",
        "            \n",
        "            # Update progress bar\n",
        "            avg_error = train_error_sum / num_train_batches * 100\n",
        "            pbar.set_description(f\"Training Error: {avg_error:.1f}%\")\n",
        "            \n",
        "            # Weakly clamped phase\n",
        "            sign = 2 * np.random.randint(0, 2) - 1  # Random sign\n",
        "            beta_signed = sign * beta\n",
        "            \n",
        "            layers_clamped = net.weakly_clamped_phase(data, target, n_it_pos, epsilon, beta_signed)\n",
        "            \n",
        "            # Compute and apply gradients\n",
        "            weight_grads, bias_grads = net.compute_gradients(layers_free, layers_clamped, beta_signed)\n",
        "            net.update_parameters(weight_grads, bias_grads, alphas)\n",
        "        \n",
        "        avg_train_error = train_error_sum / num_train_batches * 100\n",
        "        training_errors.append(avg_train_error)\n",
        "        \n",
        "        # Validation phase (subset)\n",
        "        val_error_sum = 0.0\n",
        "        num_val_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(test_loader):\n",
        "                if batch_idx >= 20:  # Only test on first 20 batches\n",
        "                    break\n",
        "                    \n",
        "                data, target = data.to(device), target.to(device)\n",
        "                \n",
        "                _, _, error = net.measure(data, target)\n",
        "                val_error_sum += error\n",
        "                num_val_batches += 1\n",
        "        \n",
        "        avg_val_error = val_error_sum / num_val_batches * 100\n",
        "        validation_errors.append(avg_val_error)\n",
        "        \n",
        "        duration = (time.time() - start_time) / 60.0\n",
        "        print(f\"  Training error: {avg_train_error:.2f}%\")\n",
        "        print(f\"  Validation error: {avg_val_error:.2f}%\")\n",
        "        print(f\"  Duration: {duration:.1f} min\\n\")\n",
        "    \n",
        "    return net, training_errors, validation_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Train the network (reduced parameters for demo)\n",
        "print(\"Starting training...\")\n",
        "\n",
        "net, train_errors, val_errors = train_network(\n",
        "    hidden_sizes=[100],  # Smaller network\n",
        "    n_epochs=3,          # Fewer epochs\n",
        "    batch_size=20,\n",
        "    n_it_neg=10,         # Fewer iterations\n",
        "    n_it_pos=4,\n",
        "    epsilon=0.5,\n",
        "    beta=0.5,\n",
        "    alphas=[0.1, 0.05],\n",
        "    device=device,\n",
        "    max_batches=50       # Limit batches per epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_results"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_errors, 'b-', label='Training Error', marker='o')\n",
        "plt.plot(val_errors, 'r-', label='Validation Error', marker='s')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error (%)')\n",
        "plt.title('Equilibrium Propagation Training Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Test on a few examples\n",
        "plt.subplot(1, 2, 2)\n",
        "test_iter = iter(test_loader)\n",
        "test_data, test_target = next(test_iter)\n",
        "test_data, test_target = test_data.to(device), test_target.to(device)\n",
        "\n",
        "# Get predictions\n",
        "predictions = net.predict(test_data[:8])  # First 8 examples\n",
        "\n",
        "# Show some examples\n",
        "for i in range(8):\n",
        "    plt.subplot(2, 4, i+1)\n",
        "    img = test_data[i].cpu().reshape(28, 28)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f'True: {test_target[i].item()}, Pred: {predictions[i].item()}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Final training error: {train_errors[-1]:.2f}%\")\n",
        "print(f\"Final validation error: {val_errors[-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explanation"
      },
      "source": [
        "## How Equilibrium Propagation Works\n",
        "\n",
        "### 1. Energy Function\n",
        "The network defines an energy function:\n",
        "```\n",
        "E(s) = (1/2) Σ ρ(s_i)² - Σ b_i ρ(s_i) - Σ W_ij ρ(s_i) ρ(s_j)\n",
        "```\n",
        "where `ρ` is the hard sigmoid activation function.\n",
        "\n",
        "### 2. Free Phase\n",
        "- Network relaxes to minimize energy: `ds/dt = -∂E/∂s`\n",
        "- Input layer is clamped to data\n",
        "- Hidden and output layers evolve to equilibrium\n",
        "\n",
        "### 3. Weakly Clamped Phase  \n",
        "- Total energy becomes: `F(s) = E(s) + β C(s)`\n",
        "- `C(s)` is cost function (e.g., MSE with targets)\n",
        "- `β` is small perturbation parameter\n",
        "- Network finds new equilibrium under this modified energy\n",
        "\n",
        "### 4. Learning Rule\n",
        "Parameter updates are proportional to:\n",
        "```\n",
        "Δθ ∝ (E_clamped - E_free) / β\n",
        "```\n",
        "\n",
        "This approximates the gradient of the cost function!\n",
        "\n",
        "### Key Insights\n",
        "- **Local learning**: Updates depend only on local equilibrium states\n",
        "- **Biologically plausible**: No need for backpropagation\n",
        "- **Energy-based**: Naturally handles deep networks\n",
        "- **Equivalence to backprop**: Under certain conditions, EP ≈ backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comparison"
      },
      "outputs": [],
      "source": [
        "# Compare with a simple baseline (random predictions)\n",
        "print(\"Comparison with random baseline:\")\n",
        "random_accuracy = 10.0  # 10% for 10-class classification\n",
        "ep_accuracy = 100 - val_errors[-1]\n",
        "\n",
        "print(f\"Random baseline accuracy: {random_accuracy:.1f}%\")\n",
        "print(f\"Equilibrium Propagation accuracy: {ep_accuracy:.1f}%\")\n",
        "print(f\"Improvement: {ep_accuracy - random_accuracy:.1f} percentage points\")\n",
        "\n",
        "# Show energy evolution during training\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_errors, 'b-', linewidth=2)\n",
        "plt.axhline(y=random_accuracy, color='r', linestyle='--', label='Random baseline')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error (%)')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Show final learned weights (first layer)\n",
        "W1 = net.weights[0].detach().cpu().numpy()\n",
        "plt.imshow(W1[:100, :100], cmap='RdBu', aspect='auto')\n",
        "plt.title('Learned Weight Matrix (subset)')\n",
        "plt.xlabel('Hidden units')\n",
        "plt.ylabel('Input pixels')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}
